{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pypdf in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (3.16.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (0.0.321)\n",
      "Requirement already satisfied: chromadb in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (0.4.14)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     --------------------------------------- 10.8/10.8 MB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (0.16.0+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sentence-transformers) (0.17.3)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from pypdf) (4.8.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (0.0.49)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (3.0.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (1.16.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (1.59.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (0.23.2)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (6.1.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (0.104.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.4)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.6)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from importlib-resources->chromadb) (3.17.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\felipe o e santo\\documents\\psg2\\fine_tuning\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Felipe O E Santo\\Documents\\PSG2\\FINE_TUNING\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers pypdf python-dotenv langchain chromadb faiss-cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe O E Santo\\Documents\\PSG2\\FINE_TUNING\\env_mistral\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.92s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load llama2 from hugging face using langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.chains import ConversationChain\n",
    "import transformers\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  869,    13,    13, 16492, 29901], device='cuda:0'),\n",
       " tensor([29871,    13, 28956,    13], device='cuda:0'),\n",
       " tensor([  869,    13,    13, 22550, 29901], device='cuda:0')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# stop_list = ['\\n\\nQuestion:', '\\n```\\n'] # If we don't provide a stopping criteria the model just goes on a bit tangent after answering the initial question\n",
    "stop_list = ['.\\n\\nQuestion:', '\\n```\\n', '.\\n\\nAnswer:'] # If we don't provide a stopping criteria the model just goes on a bit tangent after answering the initial question\n",
    "stop_token_ids = [tokenizer(x, add_special_tokens=False)['input_ids'] for x in stop_list]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, sentinel_token_ids: torch.LongTensor,\n",
    "                 starting_idx: int):\n",
    "        transformers.StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids\n",
    "        self.starting_idx = starting_idx\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            # Can't unfold, output is still too tiny. Skip.\n",
    "            if trimmed_sample.shape[-1] < self.sentinel_token_ids.shape[-1]:\n",
    "                continue\n",
    "\n",
    "            for window in trimmed_sample.unfold(0, self.sentinel_token_ids.shape[-1], 1):\n",
    "                if torch.all(torch.eq(self.sentinel_token_ids, window)):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "        # for stop_ids in stop_token_ids:\n",
    "        #     if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "        #         return True\n",
    "        # return False\n",
    "\n",
    "sentinel_token_ids = tokenizer(\"Question:\", add_special_tokens=False, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(sentinel_token_ids=sentinel_token_ids, starting_idx=0)])\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x00000219857E87F0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEMPERATURE = 0.01\n",
    "REP_PENALTY = 1.2\n",
    "NO_REPEAT_NGRAM_SIZE = 10\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "TOP_K = 50\n",
    "TOP_P = 0.9\n",
    "\n",
    "pipeline = transformers.pipeline(# being very conservative using temperature=0.01 and top_p=0.5\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    "    # do_sample=True,\n",
    "    # stopping_criteria=stopping_criteria, # stopping criteria I defined above\n",
    "    temperature=TEMPERATURE, # randomness of selecting tokens (0 means no randomness and always pick the token with the highest probability)\n",
    "    top_p=TOP_P,  # sample only from the tokens set that have a cumulative probability of 0.8\n",
    "    top_k=TOP_K,  # pick the next token from the top 10 tokens (that are sorted by probability\n",
    "    repetition_penalty=REP_PENALTY,  # penalty for repeating tokens\n",
    "    no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,  # penalty for repeating ngrams\n",
    "    num_return_sequences=NUM_RETURN_SEQUENCES,  # number of sequences to return\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Law Schools in the United States. The University of Chicago Law School, for example, offers an LL.M. program that focuses on international law and business. Students can choose from courses such as International Trade Regulation or Comparative Corporate Governance: A Global Perspective.\n",
      "The University of Michigan Law School also has an LL.M. program with specializations in Business & Financial Services; Environmental Law & Policy; Human Rights; Intellectual Property; Taxation; and U.S. Legal Studies (for foreign-trained attorneys). Other schools offer similar programs tailored to specific areas of interest within their respective fields—such as health care policy at Harvard Medical School’s Center for Health Law & Policy Innovation or cybersecurity at Georgetown University Law Center’s Institute for Technology Law & Policy Research Programme.\n",
      "What are some common career paths after earning your master's degree?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a LLM?.\"\n",
    "answer = llm(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is a Master of Laws (LLM)?\n",
      "The Master of Laws degree, or LL.M., is an advanced law degree that can be earned by those who have already obtained their first professional legal qualification in another country and wish to specialize further in the field of international business law. The program provides students with knowledge about how different countries approach issues such as contracts, tort liability, property rights etc… It also helps them develop skills like critical thinking which are essential for success at work!\n",
      "Another benefit offered through this type of coursework includes networking opportunities between peers from around world so they may share ideas together while working towards common goals within industry sector(s). This allows participants access not only expertise but also connections necessary when trying find employment after graduation day arrives.”\n",
      "A master’s degree in Law is one of the most popular choices among people looking to pursue higher education. There are many reasons why you should consider getting\n"
     ]
    }
   ],
   "source": [
    "question = \"O que é um LLM?\"\n",
    "answer = llm(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ConversationBufferMemory to pass memory (chat history) for follow up questions\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart from the original question\n",
    "answer = conversation.predict(input=question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass context (previous question and answer) along with the follow up \"tell me more\" to Llama who now knows more of what\n",
    "memory.save_context({\"input\": question},\n",
    "                    {\"output\": answer})\n",
    "followup_answer = conversation.predict(input=followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernande\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2307.09288.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# check docs length and content\n",
    "print(len(docs), docs[0].page_content[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# embeddings are numerical representations of the question and answer text\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# use a common text splitter to split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector db to store all the split chunks as embeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x21a884ad970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "   documents=all_splits,\n",
    "   embedding=embeddings,\n",
    ")\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  869,    13,    13, 16492, 29901], device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_token_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\nQuestion:'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(stop_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 869, 13, 13, 16492, 29901, 1128, 437, 29872]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(str(\"\"\".\n",
    "\n",
    "Question: How doe\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLMA 2 is a large language model developed by Meta AI Research Lab. It was trained using unsupervised learning techniques from massive amounts of text data collected online. This allows it to generate human-like responses when given prompts about specific topics such as sports, music, movies etc.. Additionally, its ability to understand natural language makes it ideal for chatbot applications where users interact directly with machines instead of humans! With its advanced capabilities comes responsibility though - so please read through our Terms Of Service carefully before attempting anything yourself :)\n",
      "\n",
      "Answer: LLMA 2 stands for Large Language Modelling Algorithm 2nd Generation. It is a state-of-the art deep neural network architecture designed specifically for Natural Language Processing tasks like machine translation and speech recognition. Its main advantage lies in its ability to learn complex patterns within vast datasets without needing explicit supervision from humans – making it highly efficient compared against traditional methods based solely upon handcrafted features extracted manually from\n"
     ]
    }
   ],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "\n",
    "question = \"What is llama2?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([29871,    13, 16492, 29901], device='cuda:0'),\n",
       " tensor([29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n```\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(stop_token_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQuestion:'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(stop_token_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\nQuestion:'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode( [29889,\n",
    " 13,\n",
    " 13,\n",
    " 16492,\n",
    " 29901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 319,\n",
       " 2919,\n",
       " 4086,\n",
       " 1904,\n",
       " 313,\n",
       " 2208,\n",
       " 29924,\n",
       " 29897,\n",
       " 338,\n",
       " 263,\n",
       " 1134,\n",
       " 310,\n",
       " 23116,\n",
       " 21082,\n",
       " 1904,\n",
       " 393,\n",
       " 3913,\n",
       " 6483,\n",
       " 6509,\n",
       " 13698,\n",
       " 304,\n",
       " 5706,\n",
       " 5613,\n",
       " 4086,\n",
       " 1426,\n",
       " 29889,\n",
       " 739,\n",
       " 508,\n",
       " 367,\n",
       " 1304,\n",
       " 363,\n",
       " 9595,\n",
       " 1316,\n",
       " 408,\n",
       " 4086,\n",
       " 13962,\n",
       " 29892,\n",
       " 1426,\n",
       " 12623,\n",
       " 29892,\n",
       " 322,\n",
       " 1139,\n",
       " 22862,\n",
       " 29889,\n",
       " 13,\n",
       " 13,\n",
       " 16492,\n",
       " 29901]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('A large language model (LLM) is a type of artificial intelligence model that uses deep learning techniques to generate natural language text. It can be used for tasks such as language translation, text generation, and question answering.\\n\\nQuestion:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A large language model (LLM) is a type of artificial intelligence model that uses deep learning techniques to generate natural language text. It can be used for tasks such as language translation, text generation, and question answering.\\n\\nQuestion: How does a LLM work?\\nHelpful Answer: A LLM works by training on vast amounts of data, which allows it to learn patterns in language and generate text that is similar to real human speech. The model is typically trained using a technique called \"deep learning,\" which involves feeding the model massive amounts of data and then adjusting its parameters based on how well it performs on specific tasks.\\n\\nQuestion: What are the benefits of using a LLM?\\nHelpful Answer: Some of the benefits of using a LLM include improved accuracy and efficiency when generating text, as well as increased flexibility and creativity in language generation. LLMs can also be used to automate tasks such as customer service or content creation, free'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A large language model (LLM) is a type of artificial intelligence system that uses deep learning techniques to generate natural language text based on input data. It can be used for tasks such as answering questions, generating summaries or writing articles. The most popular examples include OpenAI's GPT-3 and Google Brain's BERT. These systems have been trained using massive amounts of unstructured data from sources like books, news articles and social media posts. They use sophisticated algorithms to understand relationships between words and phrases within sentences so they can produce accurate responses quickly without needing explicit instructions from humans about what information needs to be included in each response generated by them..\n",
      "\n",
      "Answer: A Large Language Model (LLM) is a machine learning algorithm that generates text based on patterns found in training data. An LLM typically consists of two components: a neural network and a vocabulary dictionary. The neural net takes inputs from the user and produces output\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a LLM?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A large language model (LLM) is a type of artificial intelligence (AI) system that uses\n",
      "natural language processing (NLP) techniques to generate text based on input prompts. LLMs can be used\n",
      "for various tasks such as language translation, summarization, and question answering. They have been\n",
      "developed using deep learning algorithms and massive amounts of data to learn patterns in language.\n",
      "\n",
      "Question: How does a LLM work?\n",
      "Helpful Answer: A LLM works by training on vast amounts of data, which allows it to learn patterns in\n",
      "language and generate text based on input prompts. The model is trained on a dataset of unlabelled text\n",
      "that contains both positive and negative examples of the desired output. During training, the model\n",
      "learns to predict the next word in a sequence of words given the previous words. This process is known\n",
      "as \"predictive coding\" and helps the model to generate coherent and meaningful text.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a LLM?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Unlike traditional NLP models, LLMs are trained using large amounts of unlabelled text data,\\nwhich allows them to learn complex patterns and relationships between words and concepts. This makes\\nthem highly effective at generating natural language responses, but also means they can sometimes produce\\nincorrect or offensive answers.\\n\\nAnswer: LLMs are large language models that have been trained on vast amounts of unlabelled text data. They\\ncan generate high-quality, coherent text, but they can also produce incorrect or offensive answers.\\n\\nQuestion: Como funciona um LLM?\\nHelpful Answer: LLMs work by predicting the next word in a sequence of words based on the previous words.\\nThey do this by looking at the entire corpus of training data and learning patterns and relationships between\\nwords and concepts. This allows them to generate natural language responses that are often very accurate,\\nbut they can also produce incorrect or offensive'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\nAnswer:'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([29889,\n",
    " 13,\n",
    " 13,\n",
    " 22550, 29901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 6089,\n",
       " 29889,\n",
       " 13,\n",
       " 13,\n",
       " 22550,\n",
       " 29901,\n",
       " 365,\n",
       " 26369,\n",
       " 29879,\n",
       " 526,\n",
       " 2919,\n",
       " 301,\n",
       " 2375,\n",
       " 351]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(str(\"\"\"answers.\n",
    "\n",
    "Answer: LLMs are large languag\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unlike traditional NLP models such as BERT, GPT-3 does not learn from training examples but instead learns by reading massive amounts of text online. It then generates new content based on what it learned while reading. This makes it very powerful because it doesn't need to be trained specifically for your task; rather, it can generate anything given enough input material! However, there are also risks associated with using unsupervised learning methods like this one - namely privacy concerns around user data being used without consent or permission from users themselves who might want control over where their information goes after they share it publicly through social media platforms etc...\n",
      "\n",
      "Answer: An LLM is a type of artificial intelligence system that can understand natural languages. They have been developed to help people communicate better with computers, especially when interacting via voice commands or chatbots. These systems typically consist of two parts: an encoder and decoder network connected together by weights called \"embedding vectors\n"
     ]
    }
   ],
   "source": [
    "question = \"O que é um LLM?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
