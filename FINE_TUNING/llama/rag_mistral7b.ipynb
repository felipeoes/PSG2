{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need latest to be able to use mistral7b. RUN IN TERMINAL\n",
    "#!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "#!pip install -q trl xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe O E Santo\\Documents\\PSG2\\FINE_TUNING\\env_mistral\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 7.99MB/s]\n",
      "Downloading (…)l-00001-of-00002.bin: 100%|██████████| 9.94G/9.94G [14:34<00:00, 11.4MB/s]\n",
      "Downloading (…)l-00002-of-00002.bin: 100%|██████████| 5.06G/5.06G [07:25<00:00, 11.4MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [22:01<00:00, 660.73s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.21s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 116/116 [00:00<00:00, 58.0kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.chains import ConversationChain\n",
    "import transformers\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28705,    13, 28792, 28748, 16289, 28793]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# stop_list = ['\\n\\nQuestion:', '\\n```\\n'] # If we don't provide a stopping criteria the model just goes on a bit tangent after answering the initial question\n",
    "stop_list = ['[/INST]'] # If we don't provide a stopping criteria the model just goes on a bit tangent after answering the initial question\n",
    "stop_token_ids = [tokenizer(x, add_special_tokens=False)['input_ids'] for x in stop_list]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, sentinel_token_ids: torch.LongTensor,\n",
    "                 starting_idx: int):\n",
    "        transformers.StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids\n",
    "        self.starting_idx = starting_idx\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            # Can't unfold, output is still too tiny. Skip.\n",
    "            if trimmed_sample.shape[-1] < self.sentinel_token_ids.shape[-1]:\n",
    "                continue\n",
    "\n",
    "            for window in trimmed_sample.unfold(0, self.sentinel_token_ids.shape[-1], 1):\n",
    "                if torch.all(torch.eq(self.sentinel_token_ids, window)):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "        # for stop_ids in stop_token_ids:\n",
    "        #     if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "        #         return True\n",
    "        # return False\n",
    "        \n",
    "        \n",
    "sentinel_token_ids = tokenizer(\"\\n[/INST]\", add_special_tokens=False, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(sentinel_token_ids=sentinel_token_ids, starting_idx=0)])\n",
    "sentinel_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "class EosTokenRewardLogitsProcessor(LogitsProcessor):\n",
    "  def __init__(self,  eos_token_id: int, max_length: int):\n",
    "    \n",
    "        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n",
    "            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n",
    "\n",
    "        if not isinstance(max_length, int) or max_length < 1:\n",
    "          raise ValueError(f\"`max_length` has to be a integer bigger than 1, but is {max_length}\")\n",
    "\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.max_length=max_length\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    cur_len = input_ids.shape[-1]\n",
    "    # start to increese the reward of the  eos_tokekn from 80% max length  progressively on length\n",
    "    for cur_len in (max(0,int(self.max_length*0.8)), self.max_length ):\n",
    "      ratio = cur_len/self.max_length\n",
    "      num_tokens = scores.shape[1] # size of vocab\n",
    "      scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]] =\\\n",
    "      scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]]*ratio*10*torch.exp(-torch.sign(scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]]))\n",
    "      scores[:, self.eos_token_id] = 1e2*ratio\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x000001E3CD8495B0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=500,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        logits_processor=[EosTokenRewardLogitsProcessor(eos_token_id=tokenizer.eos_token_id, max_length=500)]\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] You are a helpful, respectful and honest assistant. Answer exactly in few words.\n",
      "What are you? [/INST] \n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "#### Prompt\n",
    "template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Answer exactly in few words.\n",
    "{question} [/INST] </s>\n",
    "\"\"\"\n",
    "\n",
    "question_p = \"\"\"What are you?\"\"\"\n",
    "context_p = \"\"\" On August 10 said that its arm JSW Neo Energy has agreed to buy a portfolio of 1753 mega watt renewable energy generation capacity from Mytrah Energy India Pvt Ltd for Rs 10,530 crore.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "response = llm_chain.run({\"question\":question_p,\"context\":context_p})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of Laws (LLB) or Juris Doctor (JD).\n",
      "\n",
      "## What is a LLM?.\n",
      "\n",
      "A Master of Laws (LLM) is a postgraduate degree that is earned after completing a Bachelor of\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a LLM?.\"\n",
    "answer = llm(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeFlag = \"cuda:0\"\n",
    "system_prompt = 'The conversation between Human and AI assisatance named Gathnex\\n'\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n{E_INST}\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-3 is a large language model developed by OpenAI. It has been trained on a massive amount of text data, including books, articles, websites, and social media posts. This allows it to generate human-like responses to prompts in various languages.\n",
      "LLMs are used for many different tasks such as translation, summarization, question answering, and more. They can be integrated into chatbots or other applications that require natural language processing capabilities.\n",
      "The potential benefits of using an LLM include improved customer service experiences due to better understanding of user intent; faster response times because the machine does not need time offline like humans do when they get tired from working too hard; increased accuracy since machines don't make mistakes like people sometimes do; cost savings since there isn't any additional staff needed after implementation etc..\n"
     ]
    }
   ],
   "source": [
    "from transformers import  TextStreamer\n",
    "\n",
    "TEMPERATURE = 0.2\n",
    "REP_PENALTY = 1.2\n",
    "NO_REPEAT_NGRAM_SIZE = 10\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "\n",
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'The conversation between Human and AI assisatance named Gathnex\\n'\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n{E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, temperature=TEMPERATURE, repetition_penalty=REP_PENALTY, no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE, num_return_sequences=NUM_RETURN_SEQUENCES)\n",
    "    \n",
    "\n",
    "stream(\"O que é um LLM?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
