{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe O E Santo\\Documents\\PSG2\\BENCHMARKING\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True | Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, set_seed\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "print(f\"GPU Available: {GPU_AVAILABLE} | Number of GPUs: {NUM_GPUS}\")\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.environ[\"HUGGINGFACE_TOKEN\"]\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "# original and tinier df, used to train first version of boto\n",
    "DATASET_NAME = \"felipeoes/filtered_qa_blue_amazon_legislation\"\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "DOCKER_VOLUME_BIND = \"workspace/data\"  # removed slash for each's workstation\n",
    "\n",
    "OUTPUT_FILE = f\"{DOCKER_VOLUME_BIND}/{DATASET_NAME}_{MODEL_NAME}.txt\"\n",
    "OUTPUT_DATASET_PATH = f\"{DOCKER_VOLUME_BIND}/{DATASET_NAME}_{MODEL_NAME}.csv\"\n",
    "\n",
    "# create output file directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FILE):\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 202310\n",
    "set_seed(SEED)\n",
    "\n",
    "# Display entire pandas column width\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.68s/it]\n",
      "c:\\Users\\Felipe O E Santo\\Documents\\PSG2\\BENCHMARKING\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using float16 for faster inference and less memory usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # use_flash_attention_2=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "# better_model = BetterTransformer.transform(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HUGGINGFACE_TOKEN)\n",
    "tokenizer.padding_side = \"left\"  # necessary for padding in batch inference\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # adjust for inference\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 250  # median length of 'abstract' column (=150)\n",
    "TOP_K = 30\n",
    "TOP_P = 0.9\n",
    "TEMPERATURE = 0.3\n",
    "REP_PENALTY = 1.2\n",
    "NO_REPEAT_NGRAM_SIZE = 10\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "DO_SAMPLE = True\n",
    "\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    top_k=TOP_K,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    repetition_penalty=REP_PENALTY,\n",
    "    no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "    num_return_sequences=NUM_RETURN_SEQUENCES,\n",
    "    do_sample=DO_SAMPLE,\n",
    ")\n",
    "\n",
    "\n",
    "def create_boto_v1_prompt(user_question: str):\n",
    "    INTRO_BLURB = (\n",
    "        \"Responda à pergunta abaixo, forneça uma resposta completa e detalhada.\"\n",
    "    )\n",
    "    INSTRUCTION_KEY = \"### Pergunta:\"\n",
    "    # INPUT_KEY = \"Input:\" # later try  with context, if necessary\n",
    "    RESPONSE_KEY = \"### Resposta:\"\n",
    "    # END_KEY = \"### Fim\"\n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{user_question}\"\n",
    "    # input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\"\n",
    "    # end = f\"{END_KEY}\"\n",
    "\n",
    "    # parts = [part for part in [blurb, instruction, response, end] if part]\n",
    "    parts = [part for part in [blurb, instruction, response] if part]\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "def apply_model_to_dataframe(\n",
    "    dataframe: pd.DataFrame,\n",
    "    model: AutoModelForCausalLM,\n",
    "    model_name: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    gen_config: GenerationConfig,\n",
    "    batch_size: int,\n",
    "    start_from: int = 0,\n",
    "):\n",
    "    generated_answers = []\n",
    "\n",
    "    # get only question that weren't answered by the model yet\n",
    "    prompts = [\n",
    "        create_boto_v1_prompt(question)\n",
    "        for index, question in enumerate(dataframe[\"question\"])\n",
    "    ]\n",
    "\n",
    "    # save dataframe every 10 rows\n",
    "    SAVE_EVERY = 1 # CHANGING TO 1 BECAUSE OF BATCH SIZE\n",
    "    fh = open(OUTPUT_FILE, \"w\")\n",
    "\n",
    "    # print prompts length to file\n",
    "    fh.write(f\"Prompts length: {len(prompts)}\")\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), file=fh):\n",
    "        start = i\n",
    "        end = start + batch_size\n",
    "        range_to_save = range(start, end)\n",
    "\n",
    "        # check if index is out of bounds and adjust range_to_save\n",
    "        if end > len(prompts):\n",
    "            range_to_save = range(start, len(prompts))\n",
    "            \n",
    "        # check if generated answers already exist for the current range\n",
    "        if dataframe[model_name].iloc[range_to_save].notnull().all():\n",
    "            continue\n",
    "\n",
    "        batch = prompts[i: i + batch_size]\n",
    "        encodings = tokenizer(batch, return_tensors=\"pt\",\n",
    "                              padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generation_outputs = model.generate(\n",
    "                **encodings,\n",
    "                generation_config=gen_config,\n",
    "            )\n",
    "        answers = tokenizer.batch_decode(\n",
    "            generation_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # concat answers to dataframe\n",
    "\n",
    "        dataframe.loc[range_to_save, model_name] = answers\n",
    "        generated_answers.extend(answers)\n",
    "\n",
    "        # save every 10 rows\n",
    "        if i % (SAVE_EVERY * batch_size) == 0:\n",
    "            dataframe.to_csv(OUTPUT_DATASET_PATH, index=False)\n",
    "            \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_index', 'file_name', 'context', 'question', 'answer', '__index_level_0__'],\n",
       "    num_rows: 15964\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME, token=HUGGINGFACE_TOKEN)\n",
    "dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe from csv workspace/data/felipeoes/filtered_qa_blue_amazon_legislation_meta-llama/Llama-2-7b-hf.csv\n",
      "Length of dataframe:  15964\n",
      "Responda à pergunta abaixo, forneça uma resposta completa e detalhada.\n",
      "\n",
      "### Pergunta:\n",
      "O que é a Amazônia Azul?\n",
      "\n",
      "### Resposta:\n"
     ]
    }
   ],
   "source": [
    "# try to load dataframe from csv\n",
    "try:\n",
    "    dataframe = pd.read_csv(OUTPUT_DATASET_PATH)\n",
    "    # create empty column if it doesn't exist\n",
    "    if MODEL_NAME not in dataframe.columns:\n",
    "        dataframe[MODEL_NAME] = np.nan\n",
    "\n",
    "    print(f\"Loaded dataframe from csv {OUTPUT_DATASET_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {OUTPUT_DATASET_PATH} not found. Creating new dataframe.\")\n",
    "    dataframe = dataset[\"train\"].to_pandas()\n",
    "    dataframe[MODEL_NAME] = np.nan\n",
    "\n",
    "print(\"Length of dataframe: \", len(dataframe))\n",
    "\n",
    "question = \"O que é a Amazônia Azul?\"\n",
    "prompt = create_boto_v1_prompt(question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_model_to_dataframe(\n",
    "    dataframe, model, MODEL_NAME, tokenizer, gen_config, BATCH_SIZE, start_from=0\n",
    ")\n",
    "keep_columns = [\"file_name\", \"context\", \"question\", \"answer\", MODEL_NAME]\n",
    "df = df[keep_columns]\n",
    "df.to_csv(OUTPUT_DATASET_PATH, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
